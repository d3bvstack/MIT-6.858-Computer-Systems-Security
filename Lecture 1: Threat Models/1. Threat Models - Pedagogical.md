# Threat Models

These notes introduce the basic vocabulary for security and show, through concrete examples, how systems fail when we get **policies**, **threat models**, or **mechanisms** wrong.

---

## 1. What Do We Mean by "Security"? (00:05:47)

**Security** = achieving some goal even when an **adversary** is trying to stop you.

- There is always a **bad guy** in the picture (attacker, adversary).
- The attacker may try to read or modify your data, impersonate you, or stop your system from working.
- A system is **secure** if it still lets the legitimate user get work done, despite what the attacker does (within the assumptions of the threat model).

Security is not about avoiding bugs entirely; it is about clearly stating:

1. What the system is supposed to protect (policy).
2. Which attackers and capabilities we are defending against (threat model).
3. How the system enforces the policy against those attackers (mechanism).

---

## 2. The Three Ingredients: Policy, Threat Model, Mechanism (00:08:50)

We structure security reasoning around three concepts:

1. **Policy** – What do we want?
2. **Threat model** – Who is the adversary and what can they do?
3. **Mechanism** – How does the system enforce the policy against that adversary?

### 2.1 Policy

A **policy** is a statement of the desired behavior of the system with respect to security. Typical policy dimensions:

- **Confidentiality** – Who is allowed to *read* a piece of data?
  - Example: Only 6.858 course staff can *read* the grades file.
- **Integrity** – Who is allowed to *modify* data or take actions?
  - Example: Only course staff can *change* grades or upload final grades to the registrar.
- **Availability** – Who should be able to use the system, and when?
  - Example: The course website should remain usable even if someone is trying to flood it with requests (DoS attack).

A good policy is **clear**, **specific**, and **matches your real goals**. If you get the policy wrong, no implementation can save you.

### 2.2 Threat Model

A **threat model** is a set of assumptions about the attacker:

- Who is the attacker?
- What goals do they have?
- What powers do they have, and what *don’t* they have?

Examples of common assumptions:

- The attacker **does not already know your password**.
- The attacker **does not have physical access** to your laptop or phone.
- The attacker **can send arbitrary network packets** to your server.

Threat models are necessary because defending against an **all-powerful** attacker (who can do anything, everywhere) is usually impossible. But they are also dangerous:

- If you **underestimate** the attacker, your system will be broken in practice.
- If you **overestimate** the attacker, you may over-engineer or make the system unusable.

Rule of thumb: be **conservative**. Assume the attacker can do more, not less, within reason.

### 2.3 Mechanism

A **mechanism** is the actual technology that enforces the policy against the assumed attacker:

- Operating system protection (user/kernel modes, page tables, file permissions).
- Cryptography (encryption, signatures, MACs).
- Application logic (authentication checks, input validation, rate-limiting, etc.).

Goal: **If** the threat model is correct **and** the mechanism is correctly implemented, **then** the policy should hold.

In practice, mechanisms are complex and buggy; many security failures we study are mechanism failures.

---

## 3. Why Security Is Hard: Negative Goals and Weakest Links (00:10:12)

Security has a special property: it is a **negative goal**.

- Positive goal example (file system): “TAs must be able to read the grades file.”
  - Easy to test: ask each TA to try reading it. If they can, you’re done.
- Negative goal example (security): “No one other than TAs can read the grades file.”
  - Much harder: you must reason about *all possible ways* a non-TA might get access
    (guessing passwords, stealing laptops, bugs in software, configuration mistakes, etc.).

Consequences:

- You must think like an attacker and search for **any** path that violates the policy.
- Defense is often an **iterative process**:
  1. Design policy, threat model, mechanism.
  2. Deploy.
  3. Someone finds a new attack (weakest link).
  4. Revise policy/threat model/mechanism.

Every real system has a **breaking point**. The question is not “Is it perfectly secure?” but:

- “Under which assumptions does it work?”
- “Where does it break, and is that acceptable given what we are protecting?”

---

## 4. When Policy Goes Wrong (00:15:05)

Getting the policy wrong means that, *by design*, your system allows actions you didn’t really want to allow.

### 4.1 Account Recovery Questions (00:15:35)

Original policy idea (e‑mail account):

- "Only the person who knows the password can log in."

Account recovery questions change the policy to:

- "You can log in if you know the password **or** the answers to some personal questions."

This **weakens** security because:

- Personal questions often have answers that are **public** or easily guessable (school, birthday, city of birth).
- Famous example: Sarah Palin's Yahoo mail account was recovered using answers found on her **Wikipedia** page.

Lesson: Recovery mechanisms **change the effective policy**. You must treat them as primary authentication, not an afterthought.

### 4.2 Interacting Policies Across Systems: The Mat Honan Attack (00:17:25)

<details>
<summary>Diagram: The Mat Honan Attack (click to expand)</summary>

![The Mat Honan Attack](Diagrams/Mat%20Honan%20Case/1.%20Mat%20Honan%20Attack.png)

</details>

Mat Honan, a Wired journalist, had multiple accounts:

- Gmail
- Apple (me.com / iCloud)
- Amazon

Each service had its own *apparently reasonable* recovery policy:

1. **Gmail**: To reset your password, Gmail could send a recovery link to a backup e‑mail address.
   - It showed a **partial** address, e.g., `***@me.com`.
2. **Apple (me.com)**: To reset, Apple required your billing address and **last 4 digits of a credit card**.
3. **Amazon**:
   - Allowed adding a **new** credit card to an account with weak checks.
   - For password reset, required **any** credit card number on the account.
   - Showed the **last 4 digits** of existing cards in the UI.

How the attacker chained these:

1. Used Amazon’s weak process to
   - Attach their own credit card to Mat’s Amazon account (no login needed to buy).
   - Use that card to reset Mat’s Amazon password.
2. Logged into Mat’s Amazon account, viewed stored card info, and read the **last 4 digits** of his *real* card.
3. Called Apple, used Mat’s public information (address) + the **last 4 digits** from Amazon to reset his me.com password.
4. With control of me.com, intercepted Gmail’s password reset e‑mail and took over Gmail.

Individually, each system’s policy sounded reasonable. Combined, they formed a **vulnerable system of systems**.

Lessons:

- Your real security boundary may span **multiple services** you do not control.
- Be wary of depending on **other sites’ policies and UIs** (e.g., showing last-4 digits) in your own policy.
- Aim for **conservative policies** that do not assume other sites will keep secrets perfectly.

---

## 5. When Threat Models Go Wrong (00:22:20)

A threat model goes wrong when you assume things about the attacker or environment that are **not actually true**.

### 5.1 Human Factors (00:22:36)

Bad assumptions that frequently show up in designs:

- “Users will pick strong, unique passwords.”
- “Users will not click on suspicious links.”
- “Users will carefully check browser address bars and TLS padlocks.”

Reality:

- Many users pick weak, re-used passwords.
- Many click through warnings and phishing links.
- Many do not understand certificate warnings or URLs.

Lesson: Avoid threat models that depend on users behaving like disciplined security experts.

### 5.2 Assumptions Change Over Time: Cryptographic Strength (00:23:29)

Threat models are **time-dependent** because hardware and attacks improve.

Example: **Kerberos and 56-bit DES keys**

- In the mid‑1980s, MIT’s Project Athena used 56‑bit DES keys in Kerberos.
- At the time, brute‑forcing 56‑bit keys was expensive and unrealistic.
- Decades later, commodity hardware made exhaustive search cheap.
- Students in 6.858 demonstrated attacks that could recover keys in roughly a day with cloud resources.

Lesson:

- A safe key size or algorithm today may be **insecure tomorrow**.
- Periodically **revisit** cryptographic assumptions (key sizes, algorithms, hash functions).

### 5.3 Stronger Attackers Than Expected: Hardware Backdoors (00:25:02)

For very strong adversaries (e.g., nation‑state intelligence agencies):

- Your threat model may need to allow for **hardware compromises**:
  - Malicious chips, firmware backdoors, altered laptops in transit.
- Public revelations about NSA capabilities suggest that certain high‑value targets cannot safely assume their hardware is pristine.

Lesson:

- Protecting against casual attackers is very different from protecting against a powerful government.
- Be realistic about **who** you are defending against; you might not be able to afford defenses against the strongest adversaries.

### 5.4 Misplaced Trust: TLS Certificate Authorities (00:26:08)

TLS/SSL security (HTTPS) relies on **Certificate Authorities (CAs)**:

- A certificate authority signs a statement: “This public key belongs to `example.com`.”
- Browsers ship with hundreds of CAs as trusted roots.
- **Any** of these CAs can issue a certificate for **any** domain.

Implicit threat model assumption:

- All CAs, worldwide, will never be compromised or misbehave.

In reality:

- Some CAs have been hacked or have issued fraudulent certificates.
- Some are controlled by governments or organizations you may not wish to fully trust.

Result:

- The **weakest CA** becomes the weakest link for the entire web PKI.

Lesson:

- Do not build systems that rely on **hundreds of independent parties** all remaining perfectly secure.
- Later techniques (certificate pinning, transparency logs, etc.) try to mitigate this.

### 5.5 Surprising Paths: The DARPA Secure OS Story (00:27:58)

DARPA funded multiple research projects to produce “secure operating systems”.

Red-team evaluation:

- DARPA hired attackers to try to break the systems *by any means*.

One OS’s failure:

- The OS itself was carefully designed and robust.
- But its **source code repository server**—a development machine—was poorly secured.
- Attackers broke into the repository, inserted a backdoor into the OS code, and waited.
- When researchers compiled the OS, it contained the backdoor, defeating its own protections.

Lesson:

- Your *supply chain* and development environment are part of the threat model.
- It’s not enough to secure the runtime system; you must secure how it is **built and distributed**.

---

## 6. When Mechanisms Go Wrong (00:29:10)

Mechanism failures occur when the actual implementation (code, configuration, protocols) does not faithfully enforce the policy, even under a correct threat model.

### 6.1 iCloud Password Guessing: Inconsistent Checks (00:29:28)

<details>
<summary>Diagram: iCloud Password Guessing (click to expand)</summary>

![iCloud Password Guessing](Diagrams/iCloud%20Password%20Guessing%20Case/2.%20iCloud%20Password%20Guessing.png)

</details>

Context: Apple’s iCloud provided multiple services and interfaces (file backup, photo sharing, “Find my iPhone”, etc.) over a common account system.

Desired policy:

- Only the correct password holder can access an account.
- Defend against online password guessing.

Typical defense:

- **Rate-limiting** login attempts (e.g., lock the account or slow down after 3–10 failures).

The bug:

- Some iCloud interfaces **enforced rate limits** on failed logins.
- The “Find my iPhone” API **forgot** to enforce any limit.
- Attackers could send **millions of guesses** via that interface.

Impact:

- Attackers could brute-force passwords for accounts that used weak or common passwords.
- Led to high-profile compromises of private data.

Lessons:

- All code paths that check credentials must implement **consistent** protections.
- In complex systems, it’s easy to miss one obscure API; systematic design and centralization of checks help.

### 6.2 Citibank’s Insecure Direct Object Reference (IDOR) (00:34:33)

Citibank provided an online interface to view credit card account details.

Workflow:

1. User logs in with username/password.
2. Site redirects to a URL like:
   - `https://citi.com/account?id=123456`.

Bug:

- The server **trusted the `id` parameter alone** and did not check that it belonged to the logged-in user.
- If an attacker changed `id=123456` to `id=123457`, they could see **another customer’s account**.

Possible root causes:

- Buggy mechanism: forgot to enforce “current user owns this account ID”.
- Flawed threat model: designer might have implicitly assumed “users will not tamper with URLs”.

Lessons:

- Never assume clients will only follow links you give them.
- Always check **authorization** on the server: “Is this user allowed to access this specific object?”

### 6.3 Android SecureRandom and Bitcoin (00:36:24)

Bitcoin security depends critically on **unpredictable private keys** and **unpredictable per-signature nonces**.

- If an attacker can predict your private key, they can steal your coins.
- If an attacker sees *two* signatures made with the same private key and the same nonce, they can often compute your private key.

On Android, many Bitcoin apps used Java’s `SecureRandom()` API to generate keys and nonces.

The intended design:

- Underneath `SecureRandom`, there is a **pseudorandom number generator (PRNG)**.
- If the PRNG is seeded with enough high-quality randomness, its outputs are hard to predict.

The bug:

- In some circumstances, Android’s implementation failed to **properly seed** the PRNG.
- The PRNG sometimes started from a **fixed or predictable seed** (e.g., all zeros).
- As a result, `SecureRandom()` emitted **predictable values**, reusing nonces across signatures.

Impact:

- Attackers could reconstruct users’ private Bitcoin keys from observed signatures.
- Many users lost their Bitcoin balances.

Lessons:

- Randomness is a **mechanism underpinning many policies**; small bugs have huge consequences.
- Do not silently ignore failures in randomness initialization.
- Cryptographic APIs must be designed so that **common uses are safe by default**.

### 6.4 Encoding Mismatches: SSL Certificates and Null Characters (00:41:00)

Certificates contain hostnames encoded in a structured binary format. One encoding pattern is:

- First, a **length byte** (how long the string is).
- Then, exactly that many bytes of the hostname (e.g., `10` followed by `Amazon.com`).

The browser implementation, written in C, typically stores strings as:

- A sequence of bytes terminated by a **null (0x00) byte**.

Attack idea:

- Suppose an attacker owns `foo.com`.
- They ask a CA for a certificate for the name:
  - `amazon.com\0x.foo.com` (where `\0` is a zero byte in the middle).
- From the CA’s perspective (length-prefixed string), the whole `amazon.com\0x.foo.com` is one valid name under `foo.com`.
- When the browser reads the bytes into a null-terminated string, it stops at the **first zero byte** and interprets the name as **`amazon.com`**.

Result:

- When visiting `https://amazon.com`, the browser might incorrectly accept the attacker’s certificate, believing it is for `amazon.com`.

Lesson:

- Encoding and parsing must be **precisely specified and consistent across all components**.
- Any mismatch between encodings (length-prefixed vs. null-terminated) can create exploitable confusion.

### 6.5 Buffer Overflows (00:45:47)

A **buffer overflow** is a classic memory safety bug, common in C programs, where code writes past the end of an allocated buffer into adjacent memory.

#### 6.5.1 Simple Vulnerable Pattern

Consider a simplified C function:

```c
int redirect() {
    char buff[128];
    int i;
    gets(buff);        // dangerous: no length check
    i = atoi(buff);
    return i;
}
```

On a typical x86 process stack:

- The function’s **local variables** (like `buff` and `i`) and **control data** (saved base pointer, return address) are placed next to each other in memory.
- `gets(buff)` has **no idea** how big `buff` is; it keeps writing until it hits a newline.

If the attacker supplies more than 128 bytes of input:

- The extra bytes overflow past `buff` into other stack data.
- In particular, they can overwrite the **saved return address**.
- When `redirect()` returns, the CPU jumps to the attacker-chosen address instead of the legitimate caller.

#### 6.5.2 Exploitation

Two main strategies:

1. **Code injection** (classic):
   - Place attacker-controlled machine code in the overflowing input (in the buffer).
   - Overwrite the return address to point into that buffer.
   - When the function returns, the injected code runs (often spawning a shell).

2. **Return-to-code (code reuse)**:
   - If modern defenses mark the stack as **non-executable**, injected code cannot run.
   - Instead, the attacker overwrites the return address to jump into existing code (e.g., library functions like `system("/bin/sh")`).
   - By carefully controlling stack contents, they can chain together useful snippets (“return-to-libc”, “ROP” attacks).

#### 6.5.3 Defenses and Limits

Common mitigations:

- **Stack canaries**: place a special value before control data; detect when overwritten.
- **Non-executable stack/heap**: mark data memory as non-executable.
- **Address Space Layout Randomization (ASLR)**: randomize code and data locations.

However:

- These mitigations **raise the bar** but do not eliminate all exploitation.
- The root cause is unsafe memory operations (e.g., `gets`, `strcpy`, unchecked array indices).

Lesson:

- Avoid unsafe functions; use safer APIs that enforce bounds.
- Design systems so that **fewer components are security-critical** (minimize the trusted computing base), so unavoidable bugs have less impact.

---

## 7. Key Takeaways and Mental Checklist (01:15:55)

To design or analyze a secure system, repeatedly ask:

1. **Policy**
   - What exactly am I trying to protect?
   - Who should be able to **read** this data?
   - Who should be able to **modify** it or act on it?
   - What are the **availability** expectations?

2. **Threat model**
   - Who are my attackers? (casual users? insiders? nation-states?)
   - What capabilities do they have? (network access, local access, physical access, legal power?)
   - What assumptions am I making about **humans** (users, admins)?
   - Are any assumptions likely to **change over time** (crypto strength, hardware trust)?

3. **Mechanism**
   - Which components actually enforce the policy? (OS kernel, application, crypto library, external service.)
   - Are there **multiple interfaces** or code paths that must all enforce the same checks?
   - Are there any places where encoding, parsing, or type conversions might disagree?
   - Can small implementation bugs (off-by-one, missing check, bad random seed) completely break security?

4. **Weakest link / composition**
   - How does my system interact with **other systems** (e.g., e‑mail, payment providers, identity providers)?
   - Could combining their policies lead to a hole, as in the Mat Honan case?

---

## 8. How to Remember This Material (end-of-lecture summary)

- **Triad mantra**: When you see a system, always say to yourself:
  - **Policy** – What is the rule?
  - **Threat model** – Who is the enemy and what can they do?
  - **Mechanism** – How is the rule enforced against that enemy?

- **Examples as anchors**:
  - Policy mistake → account recovery questions; Mat Honan.
  - Threat model mistake → human factors, DES key sizes, CAs, DARPA OS.
  - Mechanism mistake → iCloud brute-force, Citibank URL `id`, Android `SecureRandom`, SSL nulls, buffer overflows.

- When you read a new paper or system description:
  1. Identify its **policy**.
  2. Extract its **threat model** from the text.
  3. Map its **mechanisms** to the policy.
  4. Try to think like an attacker: where are the **edge cases** and weakest links?

If you practice this decomposition on each system you see, thinking in terms of **policy, threat model, and mechanism** will become automatic, and the examples above will serve as memorable patterns for the kinds of mistakes to avoid.